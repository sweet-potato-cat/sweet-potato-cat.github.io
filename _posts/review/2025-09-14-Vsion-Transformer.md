---
layout: post
title: "Vision-Transformer"
subtitle: "an image is worth 16x16 words transformers for image recognition at scale"
category: review
tags: ai
image:
  path: /assets/img/PaperReview/vit_architecture.jpg
---
## 개요
## 선행연구
* Transformer architecture (Vaswani et al., 2017)
* Self-Atterntion mechanism
## 모델
![Model Architecture](/assets/img/PaperReview/vit_architecture.jpg)
## 가설
CNN 고유의 inductive bias 없이도, 대규모 데이터로 사전학습한다면 CNN 의존이 없는 Transformer만으로도 이미지 인식에서 SOTA 성능을 달성할 수 있다
## 실험 과정 및 결과
## 나의 생각 및 궁금증
## 결론

